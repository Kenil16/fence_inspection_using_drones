# -*- coding: utf-8 -*-
"""Copy of Copy of Copy of R_CNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XBh38HrDMloq3-sD5UzQVcv8u1sBpQRV
"""

#Mask R-CNN GitHub Repository
!pip install mrcnn

#Use these specific versions otherwise problems arises
!pip install keras==2.2.5 
!pip install tensorflow-gpu==1.15.4


import sys
import time 
import tensorflow as tf
import os
from os import listdir
from xml.etree import ElementTree
from numpy import zeros
from numpy import asarray
from mrcnn.utils import Dataset
from matplotlib import pyplot
from mrcnn.visualize import display_instances
from mrcnn.utils import extract_bboxes
from mrcnn.config import Config
import mrcnn.model as modellib
from mrcnn.model import MaskRCNN
from mrcnn.utils import Dataset
from mrcnn.utils import compute_ap
from mrcnn.model import load_image_gt
from mrcnn.model import mold_image
from numpy import expand_dims
from numpy import mean
from matplotlib.patches import Rectangle
from mrcnn import visualize
import random
import cv2

import random
import itertools
import colorsys

#Load pretrained weights 
!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5 &>/dev/null

#Load trained weigths
!wget -O trained_weight https://www.dropbox.com/sh/yfns1nj9k5ksmc4/AAAsoPd8VHUmE04bSGu3xZjwa?dl=0
!unzip trained_weight
!rm trained_weight

#Load videos 
!wget -O fences https://www.dropbox.com/sh/elcjipz6vb651nk/AACdPipBvxnc-IfqEALbF20aa?dl=0
!unzip -q fences
!rm fences

##Weights and images are currently on dropbox. These might be removed hoever. Use the script for data augmentation 
##creation instead along with the 465 original images given on github 

#Load small dataset 
!wget -O fences https://www.dropbox.com/sh/stu84velgd8kxxj/AAAh6d91HK3CTtUPZEAIecNca?dl=0
!unzip -q fences
!rm fences

#Load big dataset almost 7000 images 
!wget -O fences https://www.dropbox.com/sh/t18fsl6p8hhgcgl/AADHZzMB8Ha009R6hb96Voqsa?dl=0
!unzip -q fences
!rm fences

!wget -O fences https://www.dropbox.com/sh/bvkr0nljaolr1qx/AAA6ht2hV5wuS0gQMq3LqTeYa?dl=0
!unzip -q fences
!rm fences

!wget -O fences https://www.dropbox.com/sh/zz05cvnrvnw1w7i/AACPYhetF1cUvcTqGLWFZ4uma?dl=0
!unzip -q fences
!rm fences

!mkdir -p fences_augmented/annotations
!mkdir -p fences_augmented/images

!rsync -auv  big_data1/annotations/* fences_augmented/annotations/ &>/dev/null
!rsync -auv  big_data2/annotations/* fences_augmented/annotations/ &>/dev/null
!rsync -auv  big_data3/annotations/* fences_augmented/annotations/ &>/dev/null

!rsync -auv  big_data1/images/* fences_augmented/images/ &>/dev/null
!rsync -auv  big_data2/images/* fences_augmented/images/ &>/dev/null
!rsync -auv  big_data3/images/* fences_augmented/images/ &>/dev/null

!rm -r big_data1 big_data2 big_data3

##Inspirations has been taken from https://machinelearningmastery.com/how-to-train-an-object-detection-model-with-keras/
##Initial setup of the network

dataset_name = 'fences_augmented'
#dataset_name = "small_dataset_fences"

def filelist(root, file_type):
    return [os.path.join(directory_path, f) for directory_path, directory_name, 
            files in os.walk(root) for f in files if f.endswith(file_type)]

class fenceDataset(Dataset):
  def load_dataset(self,dataset_dir,is_train=True):
    self.add_class("dataset", 1, "breach")
    images_dir = dataset_dir + '/images/'
    annotations_dir = dataset_dir + '/annotations/'
    for filename in filelist(annotations_dir,'xml'):

      root = ElementTree.parse(filename).getroot()
      image_id = int(root.find('.//id').text)

			#Make 80% of images as training set (>=5580) >=(372)
      if is_train and int(image_id) >= 5580:
        continue
			#Make 20% as validation/test set (<5580) <(372)
      if not is_train and int(image_id) < 5580:
        continue
      img_path = images_dir + '/image' + str(image_id) + '.png'
      ann_path = filename #annotations_dir + str(image_id) + '.xml'
      self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)
 
  def extract_boxes(self, filename):
		
    tree = ElementTree.parse(filename)
    root = tree.getroot()
    boxes = list()
    for box in root.findall('.//bndbox'):
      xmin = int(box.find('xmin').text)
      ymin = int(box.find('ymin').text)
      xmax = int(box.find('xmax').text)
      ymax = int(box.find('ymax').text)
      coors = [xmin, ymin, xmax, ymax]
      boxes.append(coors)
    width = int(root.find('.//size/width').text)
    height = int(root.find('.//size/height').text)
    return boxes, width, height
    
  def load_mask(self, image_id):
    
    info = self.image_info[image_id]
    path = info['annotation']
    boxes, w, h = self.extract_boxes(path)
    masks = zeros([h, w, len(boxes)], dtype='uint8')
    class_ids = list()

    for i in range(len(boxes)):
      box = boxes[i]
      row_s, row_e = box[1], box[3]
      col_s, col_e = box[0], box[2]
      masks[row_s:row_e, col_s:col_e, i] = 1
      class_ids.append(self.class_names.index('breach'))
    return masks, asarray(class_ids, dtype='int32')
    
  def image_reference(self, image_id):
    info = self.image_info[image_id]
    return info['path']


class fenceConfig(Config):
  NAME = "fence_cfg"
  NUM_CLASSES = 1 + 1
  STEPS_PER_EPOCH = 100
  GPU_COUNT = 1
  IMAGES_PER_GPU = 1
  DETECTION_MIN_CONFIDENCE = 0.7


class PredictionConfig(Config):
  NAME = "fence_cfg"
  NUM_CLASSES = 1 + 1
  GPU_COUNT = 1
  IMAGES_PER_GPU = 1
  DETECTION_MIN_CONFIDENCE = 0.7
 

def evaluate_model(dataset, model, cfg):
  APs = list()
  for image_id in dataset.image_ids:
    image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)
    #Convert pixel values (e.g. center)
    scaled_image = mold_image(image, cfg)
    #Convert image into one sample
    sample = expand_dims(scaled_image, 0)
    #Make prediction
    yhat = model.detect(sample, verbose=0)
    #Extract results for first sample
    r = yhat[0]
    #Calculate statistics, including mean average presicion (mAP)
    AP, precisions, recalls, overlaps = new_compute_ap(gt_bbox, gt_class_id, gt_mask, r["rois"], r["class_ids"], r["scores"], r['masks'])
    #Because the algorithm sometime returns 'nan'
    if not math.isnan(AP):
      APs.append(AP)
  #Calculate the mean AP across all images
  mAP = mean(APs)
  return mAP

train_set = fenceDataset()
train_set.load_dataset(dataset_name, is_train=True)
train_set.prepare()
print('Train: %d' % len(train_set.image_ids))

image_id = 0
image = train_set.load_image(image_id)
mask, class_ids = train_set.load_mask(image_id)
bbox = extract_bboxes(mask)

display_instances(image, bbox, mask, class_ids, train_set.class_names)

#Train the network giving a dataset
train_set = fenceDataset()
train_set.load_dataset(dataset_name, is_train=True)
train_set.prepare()
print('Train: %d' % len(train_set.image_ids))
 
test_set = fenceDataset()
test_set.load_dataset(dataset_name, is_train=False)
test_set.prepare()
print('Test: %d' % len(test_set.image_ids))

config = fenceConfig()
cfg  = PredictionConfig()

model = MaskRCNN(mode='training', model_dir='./', config=config)
model_inference = MaskRCNN(mode='training', model_dir='./', config=cfg )

model.load_weights('mask_rcnn_coco.h5', by_name=True, exclude=["mrcnn_class_logits", "mrcnn_bbox_fc",  "mrcnn_bbox", "mrcnn_mask"])

t = time.localtime()
current_time = time.strftime("%H:%M:%S", t)

#Now redirect stdout to file for plotting
stdout = sys.stdout

sys.stdout = open('loses_outputs' + str(current_time) + '.txt', 'w')
model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=50, layers='all')

#Redirect stdout back
sys.stdout = stdout

#Read output from loses to make inference 
loss_names = ['loss', 'rpn_class_loss', 'rpn_bbox_loss', 'mrcnn_class_loss', 'mrcnn_bbox_loss',\
              'mrcnn_mask_loss','val_loss', 'val_rpn_class_loss', 'val_rpn_bbox_loss',\
              'val_mrcnn_class_loss', 'val_mrcnn_bbox_loss', 'val_mrcnn_mask_loss']
f = open('loses_outputs.txt','r')

lines = f.readlines()
loses = []

for i in lines:
  loss = []
  item = i.split()
  for index in range (len(item)):
    if item[0] == '100/100':
      if item[index] == "loss:":
        loss.append(float(item[index+1]))
      if item[index] == "rpn_class_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "rpn_bbox_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "mrcnn_class_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "mrcnn_bbox_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "mrcnn_mask_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "val_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "val_rpn_class_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "val_rpn_bbox_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "val_mrcnn_class_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "val_mrcnn_bbox_loss:":
        loss.append(float(item[index+1]))
      if item[index] == "val_mrcnn_mask_loss:":
        loss.append(float(item[index+1]))
  if len(loss):
    loses.append(loss)

print(loses)

#Print inference
t = time.localtime()
current_time = time.strftime("%H:%M:%S", t)

loss_index = [5,11]

fig, ax = pyplot.subplots()
    

ax.set_axisbelow(True)
ax.set_facecolor('#E6E6E6')
pyplot.grid(color='w', linestyle='solid')

for spine in ax.spines.values():
    spine.set_visible(False)

ax.xaxis.tick_bottom()
ax.yaxis.tick_left()

ax.tick_params(colors='gray', direction='out')
for tick in ax.get_xticklabels():
  tick.set_color('black')
for tick in ax.get_yticklabels():
  tick.set_color('black')

epochs = range(len(loses))
ax.plot(epochs, [l[loss_index[0]] for l in loses],label=loss_names[loss_index[0]])
ax.plot(epochs, [l[loss_index[1]] for l in loses],label=loss_names[loss_index[1]])

ax.set_title("Loss in {} epochs".format(len(loses)))

ax.legend()

pyplot.xlabel('Epoch')
pyplot.ylabel('Loss')
pyplot.savefig(str(current_time) + '.png')

!rm -r fence_cfg20201220T1349/ fence_cfg20201220T1359/ fence_cfg20201220T1409/ fence_cfg20201220T1422/

#Evaluate the mask-rcnn to find the mAP
train_set = fenceDataset()
train_set.load_dataset(dataset_name, is_train=True)
train_set.prepare()
print('Train: %d' % len(train_set.image_ids))

test_set = fenceDataset()
test_set.load_dataset(dataset_name, is_train=False)
test_set.prepare()
print('Test: %d' % len(test_set.image_ids))

sess = tf.Session()
file_writer = tf.summary.FileWriter('./', sess.graph)

cfg = PredictionConfig()

model = MaskRCNN(mode='inference', model_dir='./', config=cfg)

path = "trained_weights/mask_rcnn_fence_cfg_0050_small_all.h5"
#path = "fence_cfg20201220T1217/mask_rcnn_fence_cfg_0050.h5"
model.load_weights(path, by_name=True)

train_mAP = evaluate_model(train_set, model, cfg)
print("Train mAP: %.3f" % train_mAP)

test_mAP = evaluate_model(test_set, model, cfg)
print("Test mAP: %.3f" % test_mAP)

#Load images from video 
!rm -r video_fences/data/trial1/
!mkdir -p video_fences/data/trial1/
!mkdir -p video_fences/data/rois_images
path_in = "video_fences/test2.mp4"
path_out = "video_fences/data/trial1/"

#Make dataset for test from video 
class fenceDataset(Dataset):
  def load_dataset(self,dataset_dir,is_train=True):
    self.add_class("dataset", 1, "breach")
    
    count = 0
    vidcap = cv2.VideoCapture(path_in)
    success, image = vidcap.read()
    success = True

    while success:
      vidcap.set(cv2.CAP_PROP_POS_MSEC,(count*1000)) 
      success, image = vidcap.read()
      if success:
        img_path = path_out + "image%d.png" % count
        cv2.imwrite(img_path, image)
        self.add_image('dataset', image_id=count, path=img_path)
        count = count + 1

#Detect breaches in the video (Final acceptence test)
train_set = fenceDataset()
train_set.load_dataset(dataset_name, is_train=True)
train_set.prepare()
print('Train: %d' % len(train_set.image_ids))

test_set = fenceDataset()
test_set.load_dataset(dataset_name, is_train=False)
test_set.prepare()
print('Test: %d' % len(test_set.image_ids))

config = fenceConfig()
cfg  = PredictionConfig()

model = MaskRCNN(mode='inference', model_dir='./', config=cfg)
path = "trained_weights/mask_rcnn_fence_cfg_0050_all.h5"
model.load_weights(path, by_name=True)

def get_ax(rows=1, cols=1, size=8):
    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))
    return ax

for i in range(len(test_set.image_ids)):
  image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(test_set, config, i, use_mini_mask=False)
  results = model.detect([image], verbose=1)
  r = results[0]
  display_instances(image, r['rois'], r['masks'], r['class_ids'],test_set.class_names, r['scores'], ax=get_ax())